{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1ad133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "import json\n",
    "import requests\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# create a client instance of the library\n",
    "#elastic_client = Elasticsearch(timeout=60, max_retries=10, retry_on_timeout=True)\n",
    "#elastic_client.cluster.health(wait_for_status='yellow', request_timeout=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ac437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_save_path = 'D:\\\\Code\\\\datascience\\\\MA_NFT\\\\data\\\\pickle\\\\elastic_df.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5fb07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickle_save_path,'rb') as path_name:\n",
    "    elastic_df = pickle.load(path_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f420bc",
   "metadata": {},
   "source": [
    "### Stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e834bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sales feature\n",
    "\n",
    "from scipy import stats \n",
    "\n",
    "elastic_df['sale_usd'].describe()\n",
    "\n",
    "mean = np.mean(elastic_df['sale_usd'])\n",
    "std = np.std(elastic_df['sale_usd'])\n",
    "median = np.median(elastic_df['sale_usd'])\n",
    "print('Mean of sales is', mean)\n",
    "print('Std. deviation is', std)\n",
    "print('Median is', median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e27450",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "outlier = []\n",
    "for i in elastic_df['sale_usd']:\n",
    "    z = (i-mean)/std\n",
    "    if z > threshold:\n",
    "        outlier.append(i)\n",
    "len(outlier)        \n",
    "\n",
    "elastic_df = elastic_df[elastic_df['sale_usd'].apply(lambda usd: usd not in outlier)]\n",
    "\n",
    "elastic_df = elastic_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5248645d",
   "metadata": {},
   "source": [
    "### Y-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_usd = np.max(elastic_df['sale_usd']) + 1 # added 1 because sometimes the max value was not bined\n",
    "print(f'Max price: {max_usd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb52ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(elastic_df['sale_usd'], bins = np.logspace(start = np.log10(1), stop = np.log10(max_usd), num = 20))\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cd4257",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.logspace(start = np.log10(1), stop = np.log10(max_usd), num = 20)\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_number(price):\n",
    "    for i in range(0, len(bins)):\n",
    "        start = 0\n",
    "        if i > 0:\n",
    "            start = bins[i-1]\n",
    "        end = bins[i]\n",
    "        if price > start and price <= end:\n",
    "            return int(i) \n",
    "        \n",
    "elastic_df['bin_log_usd'] = elastic_df['sale_usd'].apply(bin_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b7459",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_labels = []\n",
    "for i in range(0, len(bins)):\n",
    "    start = 0\n",
    "    if i > 0:\n",
    "        start = bins[i-1]\n",
    "    end = bins[i]\n",
    "    s = '{:.1f}'.format(start)\n",
    "    e = '{:.1f}'.format(end)\n",
    "    bin_labels.append(f'{s}-{e}')\n",
    "bin_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4511b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating histogram\n",
    "fig, ax = plt.subplots(1, 1, figsize=(30,20))\n",
    "ax.hist(elastic_df['bin_log_usd'], bins=len(bin_labels), align='mid')\n",
    "# Set title\n",
    "ax.set_title(\"USD\")\n",
    "# adding labels\n",
    "ax.set_xlabel('x-label')\n",
    "ax.set_ylabel('y-label')\n",
    "# Make some labels.\n",
    "\n",
    "#range(len(bin_labels)\n",
    "rects = ax.patches\n",
    "for rect, label in zip(rects, bin_labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width() / 2, height+0.01, label,\n",
    "            ha='center', va='bottom', fontsize=20)\n",
    "plt.xticks(fontsize=0)\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdd8222",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_df['cat_usd'] = elastic_df['bin_log_usd'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec328b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {\n",
    "    idx: class_\n",
    "    for idx, class_ in enumerate(bin_labels)\n",
    "}\n",
    "class_to_idx = {bin_labels[i]: i for i in range(len(bin_labels))}\n",
    "\n",
    "class_to_idx\n",
    "# Restart for ResNet\n",
    "# Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ded21a",
   "metadata": {},
   "source": [
    "### Split on X&Y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df256f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bcf1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = elastic_df['cat_usd'].copy()\n",
    "X = elastic_df[['contract_scheme', \n",
    "                'sale_time', 'collection_created_year', 'unique_asset', 'instagram_account',\n",
    "                'twitter_account', 'name_tok', 'creator_tok', 'collection_name_tok', 'instagram_tok', \n",
    "                'twitter_tok', 'id', 'preview_path', 'img_path',\n",
    "                'word_count_coll_desc', 'word_count_descr', 'z_twitter_follower']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5163b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = ['contract_scheme', 'unique_asset', 'instagram_account',\n",
    "                'twitter_account', 'name_tok', 'creator_tok', 'collection_name_tok', 'instagram_tok', \n",
    "                'twitter_tok']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in cat_col:\n",
    "    X[category] = X[category].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5107177",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = ['sale_time', 'collection_created_year', 'word_count_coll_desc', 'word_count_descr', 'z_twitter_follower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = scaler.fit_transform(X[num_col])\n",
    "for i, col in enumerate(X[num_col].columns):\n",
    "    newCol = []\n",
    "    for ii in range(0, len(scaled)):\n",
    "        newCol.append(scaled[ii][i])\n",
    "    X[col] = newCol\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7d8133",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['id'] = elastic_df['id']\n",
    "X['preview_path'] = elastic_df['preview_path']\n",
    "X['img_path'] = elastic_df['img_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b4dbce",
   "metadata": {},
   "source": [
    "### Textual cat features transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c5868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_cat_values(cat, number):\n",
    "    #tops = cat.value_counts()[:number].index.tolist()\n",
    "    tops = cat.value_counts()[:number - 1].index.tolist()\n",
    "    default_cat = number - 1\n",
    "    #[f(x) if condition else g(x) for x in sequence]\n",
    "    arr = [tops.index(x) if x in tops else default_cat for x in cat]\n",
    "    return pandas.Series(arr).astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['instagram_tok'] = top_cat_values(X['instagram_tok'], 4096)\n",
    "X['name_tok'] = top_cat_values(X['name_tok'], 4096)\n",
    "X['collection_name_tok'] = top_cat_values(X['collection_name_tok'], 4096)\n",
    "X['twitter_tok'] = top_cat_values(X['twitter_tok'], 4096)\n",
    "X['creator_tok'] = top_cat_values(X['creator_tok'], 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89945be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_column_sizes = [len(X[column].cat.categories) for column in cat_col]\n",
    "categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in categorical_column_sizes]\n",
    "print(categorical_embedding_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ad6df",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90125e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.5\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X,Y, train_size = train_size, random_state=69)\n",
    "\n",
    "# Now since we want the valid and test size to be equal (20% each of overall data). \n",
    "# we have to define valid_size = 0.5 (that is 50% of remaining data)\n",
    "test_size = 0.5\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size = 0.5, random_state=42)\n",
    "\n",
    "print(X_train.shape), print(y_train.shape)\n",
    "print(X_valid.shape), print(y_valid.shape)\n",
    "print(X_test.shape), print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef021f4",
   "metadata": {},
   "source": [
    "### Prep tab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_cat(dataframe):\n",
    "    cat_col = ['contract_scheme', 'unique_asset', 'instagram_account', 'twitter_account']\n",
    "    cat_col2 = ['name_tok', 'creator_tok', 'collection_name_tok', 'instagram_tok', 'twitter_tok']\n",
    "    arr = []\n",
    "    for col in cat_col:\n",
    "        arr.append(dataframe[col].cat.codes.values)\n",
    "    for col in cat_col2:\n",
    "        arr.append(dataframe[col].values)\n",
    "        \n",
    "    stack_col = np.stack(arr, 1)\n",
    "    return torch.tensor(stack_col, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95267674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_num(dataframe):\n",
    "    num_col = ['sale_time', 'collection_created_year', 'word_count_coll_desc', 'word_count_descr', 'z_twitter_follower']\n",
    "    arr = []\n",
    "    for col in num_col:\n",
    "        arr.append(dataframe[col].values)\n",
    "    stack_col = np.stack(arr, 1)\n",
    "    \n",
    "    return torch.tensor(stack_col, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e85d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = stack_cat(X_train).to(device)\n",
    "train_num = stack_num(X_train).to(device)\n",
    "\n",
    "#del X_train\n",
    "\n",
    "valid_cat = stack_cat(X_valid).to(device)\n",
    "valid_num = stack_num(X_valid).to(device)\n",
    "\n",
    "#del X_valid\n",
    "\n",
    "test_cat = stack_cat(X_test).to(device)\n",
    "test_num = stack_num(X_test).to(device)\n",
    "\n",
    "#del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef1f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_prep(dataframe_y):\n",
    "    dataframe_y = torch.tensor(dataframe_y).flatten()\n",
    "    return dataframe_y    \n",
    "\n",
    "y_train = output_prep(y_train.values).to(device)\n",
    "y_valid = output_prep(y_valid.values).to(device)\n",
    "y_test = output_prep(y_test.values).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d61d60",
   "metadata": {},
   "source": [
    "### Pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pickle \n",
    "\n",
    "Save\n",
    "\n",
    "pickle_save_path_X_train = 'D:\\\\Code\\\\datascience\\\\MA_NFT\\\\data\\\\pickle\\\\X_train.pkl'\n",
    "with open(pickle_save_path_X_train,'wb') as path_name:\n",
    "    pickle.dump(X_train, path_name) \n",
    "    \n",
    "pickle_save_path_y_train = 'D:\\\\Code\\\\datascience\\\\MA_NFT\\\\data\\\\pickle\\\\y_train.pkl'\n",
    "with open(pickle_save_path_y_train,'wb') as path_name:\n",
    "    pickle.dump(y_train, path_name)\n",
    "    \n",
    "pickle_save_path_X_valid = 'D:\\\\Code\\\\datascience\\\\MA_NFT\\\\data\\\\pickle\\\\X_valid.pkl'\n",
    "with open(pickle_save_path_X_valid,'wb') as path_name:\n",
    "    pickle.dump(X_valid, path_name)\n",
    "    \n",
    "pickle_save_path_y_valid = 'D:\\\\Code\\\\datascience\\\\MA_NFT\\\\data\\\\pickle\\\\y_valid.pkl'\n",
    "with open(pickle_save_path_y_valid,'wb') as path_name:\n",
    "    pickle.dump(y_valid, path_name)\n",
    "    \n",
    "pickle_save_path_X_test = 'D:\\\\Code\\\\datascience\\\\MA_NFT\\\\data\\\\pickle\\\\X_test.pkl'\n",
    "with open(pickle_save_path_X_test,'wb') as path_name:\n",
    "    pickle.dump(X_test, path_name)\n",
    "    \n",
    "pickle_save_path_y_test = 'D:\\\\Code\\\\datascience\\\\MA_NFT\\\\data\\\\pickle\\\\y_test.pkl'\n",
    "with open(pickle_save_path_y_test,'wb') as path_name:\n",
    "    pickle.dump(y_test, path_name) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
